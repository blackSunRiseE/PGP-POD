{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install mpi4py"
      ],
      "metadata": {
        "id": "LXacK7Xcjx2e",
        "outputId": "39a1525f-0541-4730-8af9-e8a686049778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 23.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp38-cp38-linux_x86_64.whl size=4438394 sha256=bfb793f5704cfe9230bf571b29f5395b9f4e342af68476241df05e37a9b39e33\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/35/48/0b9a7076995eea5ea64a7e4bc3f0f342f453080795276264e7\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hiyDUqlRjqqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102d522f-fc66-437c-9330-91fd261e5838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting DataSet.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile DataSet.py\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "train_data = datasets.MNIST(\n",
        "      root = 'data',\n",
        "      train = True,                         \n",
        "      transform = ToTensor(), \n",
        "      download = True,            \n",
        "  )\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile DataLoader.py\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from DataSet import train_data,test_data\n",
        "def DataLoader(batchSize, numWorkers,shuffle = False,):\n",
        "  loaders = {\n",
        "  'train' : torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=batchSize, \n",
        "                                          shuffle=shuffle, \n",
        "                                          num_workers=numWorkers),\n",
        "  \n",
        "  'validate'  : torch.utils.data.DataLoader(test_data, \n",
        "                                          batch_size=batchSize, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=numWorkers)\n",
        "  }\n",
        "  return loaders     \n"
      ],
      "metadata": {
        "id": "T4fCyHcdj076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c4a11e-ea85-4c36-d840-03f35ee1aef4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting DataLoader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Model.py\n",
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "IJf14P0Fj2Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b49a6de-a115-40be-c502-2f3fc00e9f99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmN16gPjtVr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CursMain.py\n",
        "from mpi4py import MPI\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from DataLoader import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from Model import CNN\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "def train_model(num_epochs,\n",
        "                criterion,\n",
        "                test_dataloader,\n",
        "                rank,\n",
        "                batch_size,\n",
        "                optimizer,\n",
        "                model,\n",
        "                train_dataloader,\n",
        "                val_dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model_best = 0\n",
        "    for epoch in range(num_epochs):\n",
        "      running_loss = 0\n",
        "      accuracy = 0\n",
        "\n",
        "      dataset_sizes_train = len(train_dataloader)\n",
        "      model.train()\n",
        "      if rank == 0:\n",
        "        i = 0\n",
        "        for images, labels in tqdm(train_dataloader):\n",
        "          comm.send((images, labels),dest = i%(p-1)+1)\n",
        "          i+=1\n",
        "      if rank != 0:\n",
        "        for i in range(len(train_dataloader)):    \n",
        "          if i % (p - 1) + 1 == rank:\n",
        "            (images,lables) = comm.recv(source=0)\n",
        "            images = images.to(device)\n",
        "            lables = lables.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output,lables)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * images.size(0)\n",
        "        running_loss = running_loss / dataset_sizes_train\n",
        "        print(\"Epoch of train:\", epoch + 1,\"Loss: [\", running_loss, \"]\", \"rank: \", rank)\n",
        "            \n",
        "      MPI.Comm.Barrier(MPI.COMM_WORLD)\n",
        "      \n",
        "      accuracy = 0\n",
        "      validate_loss = 0.0\n",
        "      dataset_sizes_val = len(val_dataloader)\n",
        "      if rank != 0:\n",
        "        model.eval()\n",
        "      if rank == 0:\n",
        "        i = 0\n",
        "        for images, labels in tqdm(val_dataloader):\n",
        "          comm.send((images,labels), dest=i % (p - 1) + 1)\n",
        "          i+=1\n",
        "      if rank != 0:\n",
        "        for i in range(len(val_dataloader)):    \n",
        "          if i % (p - 1) + 1 == rank:\n",
        "            (images,lables) = comm.recv(source=0)\n",
        "            images = images.to(device)\n",
        "            lables = lables.to(device)\n",
        "            with torch.no_grad():\n",
        "              output = model(images)\n",
        "              loss = criterion(output,lables)\n",
        "              validate_loss += loss.item() * images.size(0)\n",
        "              pred_y = torch.max(output, 1)[1].data.squeeze()\n",
        "        validate_loss = validate_loss / dataset_sizes_val\n",
        "        print(\"Epoch of validation:\", epoch + 1,\"Loss: \",validate_loss, rank)\n",
        "      MPI.Comm.Barrier(MPI.COMM_WORLD) \n",
        "      if rank != 0: \n",
        "        if epoch == 0:\n",
        "          model_best = validate_loss\n",
        "        if validate_loss <= model_best:\n",
        "          model_best = validate_loss\n",
        "          torch.save(model.state_dict(), f\"./weights/model_{rank}.pth\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  comm = MPI.COMM_WORLD\n",
        "  my_rank = comm.Get_rank()\n",
        "  p = comm.Get_size()\n",
        "  num_epochs = 3\n",
        "  batch_size = 30\n",
        "  num_workers = 4\n",
        "  train_dataloader = DataLoader(batch_size,num_workers)['train']\n",
        "  validate_dataloader = DataLoader(batch_size,num_workers)['validate']\n",
        "  model = CNN()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = train_model(num_epochs,criterion,validate_dataloader,my_rank,\n",
        "                           batch_size,optimizer,model,train_dataloader,validate_dataloader)\n",
        "  MPI.Finalize                         \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG2QUcA4j91y",
        "outputId": "e80b9bb8-a955-4682-f8f0-8cbc6745e89b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CursMain.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root -np 4 python CursMain.py"
      ],
      "metadata": {
        "id": "PJvktL2fj_M2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eed3ed7-0c8c-49ca-c155-efbd971d1194"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 99%|█████████▉| 1982/2000 [00:20<00:00, 129.98it/s]Epoch of train: 1 Loss: [ 3.174856982465135 ] rank:  3\n",
            "100%|█████████▉| 1999/2000 [00:20<00:00, 141.31it/s]Epoch of train: 1 Loss: [ 2.8188923583982977 ] rank:  1\n",
            "Epoch of train: 1 Loss: [ 2.93712607084075 ] rank:  2\n",
            "100%|██████████| 2000/2000 [00:20<00:00, 97.10it/s] \n",
            " 98%|█████████▊| 326/334 [00:02<00:00, 147.20it/s]Epoch of validation: 1 Loss:  0.9887085081507524 2\n",
            "Epoch of validation: 1 Loss:  1.1480538361541324 3\n",
            "Epoch of validation: 1 Loss:  0.9974066961227643 1\n",
            "100%|██████████| 334/334 [00:02<00:00, 130.95it/s]\n",
            " 99%|█████████▉| 1983/2000 [00:15<00:00, 126.77it/s]Epoch of train: 2 Loss: [ 0.9541301033759373 ] rank:  3\n",
            "100%|█████████▉| 1998/2000 [00:15<00:00, 132.07it/s]Epoch of train: 2 Loss: [ 0.8603720277588581 ] rank:  1\n",
            "Epoch of train: 2 Loss: [ 0.8783133789914427 ] rank:  2\n",
            "100%|██████████| 2000/2000 [00:15<00:00, 128.25it/s]\n",
            " 93%|█████████▎| 312/334 [00:02<00:00, 158.94it/s]Epoch of validation: 2 Loss:  0.7483845197562196 2\n",
            "Epoch of validation: 2 Loss:  0.7985677917816949 3\n",
            "Epoch of validation: 2 Loss:  0.6488962395989453 1\n",
            "100%|██████████| 334/334 [00:02<00:00, 134.10it/s]\n",
            " 99%|█████████▉| 1982/2000 [00:15<00:00, 133.40it/s]Epoch of train: 3 Loss: [ 0.6202735600852247 ] rank:  3\n",
            "Epoch of train: 3 Loss: [ 0.5842499267905805 ] rank:  1\n",
            "100%|██████████| 2000/2000 [00:15<00:00, 146.14it/s]Epoch of train: 3 Loss: [ 0.5705852943277568 ] rank:  2\n",
            "100%|██████████| 2000/2000 [00:15<00:00, 131.60it/s]\n",
            " 96%|█████████▌| 320/334 [00:02<00:00, 157.84it/s]Epoch of validation: 3 Loss:  0.5884649736031577 2\n",
            "Epoch of validation: 3 Loss:  0.6797634261262235 3\n",
            "Epoch of validation: 3 Loss:  0.47361320262833384 1\n",
            "100%|██████████| 334/334 [00:02<00:00, 135.35it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Test.py\n",
        "from mpi4py import MPI\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from DataLoader import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from Model import CNN\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "def test(model, criterion, dataloader_test, dataset_sizes_test):\n",
        "    score = 0\n",
        "    runing_loss = 0.0\n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        if rank != 0:\n",
        "            print(\"Start proccess number \",rank)\n",
        "            for image, label in tqdm(dataloader_test): \n",
        "                output = model(image)\n",
        "                comm.send(output, dest=0, tag=0) \n",
        "                if rank == 1:\n",
        "                    comm.send(label, dest=0, tag=1)\n",
        "                _, preds = torch.max(output, 1)\n",
        "                loss = criterion(output, label)\n",
        "                runing_loss += loss.item() * image.size(0)\n",
        "                score += torch.sum(preds == label.data)\n",
        "            epoch_acc = score.double() / dataset_sizes_test\n",
        "            runing_loss = runing_loss / dataset_sizes_test\n",
        "            print(\"Test process \", rank, \": score: [\", epoch_acc.item(), \"], loss: [\", runing_loss, \"]\")\n",
        "        \n",
        "        MPI.Comm.Barrier(MPI.COMM_WORLD)\n",
        "        result = 0\n",
        "        if rank == 0:\n",
        "            print(\"Start proccess number \",rank)\n",
        "            for i in tqdm(range(len(dataloader_test))):\n",
        "                label = comm.recv(source=1, tag=1) \n",
        "                for procid in range(1, p):\n",
        "                    output = comm.recv(source=procid, tag=0)\n",
        "                    if procid == 1:\n",
        "                        result_all_models = output\n",
        "                    else:\n",
        "                        result_all_models += output\n",
        "                _, preds = torch.max(result_all_models, 1)\n",
        "                result += torch.sum(preds == label.data)\n",
        "            result = result.double() / dataset_sizes_test\n",
        "            print(\"Test process result\", rank, result.item())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  comm = MPI.COMM_WORLD\n",
        "  rank = comm.Get_rank()\n",
        "  p = comm.Get_size()\n",
        "  num_epochs = 3\n",
        "  batch_size = 30\n",
        "  num_workers = 4\n",
        "  train_dataloader = DataLoader(batch_size,num_workers)['train']\n",
        "  validate_dataloader = DataLoader(batch_size,num_workers)['validate']\n",
        "  model = CNN()\n",
        "  if(rank != 0):\n",
        "     model.load_state_dict(torch.load(f'/content/weights/model_{rank}.pth'))\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = test(model,criterion,validate_dataloader,len(validate_dataloader) * batch_size)\n",
        "  MPI.Finalize    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmgyIy-5pzfC",
        "outputId": "444f4964-a2ff-49d9-f226-41153726e8f7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root -np 4 python Test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWFXg5cdvc0p",
        "outputId": "73a6be16-2be0-45bf-8aaa-eaa24deee7a6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start proccess number  1\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  4%|▍         | 14/334 [00:00<00:16, 19.44it/s]Start proccess number  2\n",
            "  5%|▌         | 17/334 [00:01<00:15, 20.03it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  4%|▍         | 14/334 [00:00<00:14, 21.79it/s]Start proccess number  3\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 334/334 [00:14<00:00, 23.55it/s]Test process  2 : score: [ 0.9755489021956087 ], loss: [ 0.06615605563419813 ]\n",
            "100%|██████████| 334/334 [00:16<00:00, 19.74it/s]Test process  1 : score: [ 0.981936127744511 ], loss: [ 0.04612626874586567 ]\n",
            "100%|██████████| 334/334 [00:15<00:00, 20.93it/s]Test process  3 : score: [ 0.9760479041916168 ], loss: [ 0.06548089371242899 ]\n",
            "\n",
            "\n",
            "Start proccess number  0\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 334/334 [00:00<00:00, 466.45it/s]Test process result 0 0.41097804391217563\n",
            "\n"
          ]
        }
      ]
    }
  ]
}